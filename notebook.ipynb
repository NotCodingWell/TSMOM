{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Thêm thư viện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các thư viện cần thiết, trong đó có `yfinance` để lấy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn import tree\n",
    "import xgboost as xgb\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from hypopt import GridSearch\n",
    "import keras_tuner as kt\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Lấy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hàm lấy data, là cổ phiếu của 50 công ty trên sàn `EURO_STOXX_50` \\\n",
    "Kết quả trả về là 1 dataframe có dạng m dòng, 50 cột với m là time range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EU_Stock_data(start_time,end_time, time_range = 'max'):\n",
    "    \"\"\"Lấy dữ liệu giá Close của 50 công ty trên sàn Euro_STOXX 50 vào thời gian cho trước\"\"\"\n",
    "\n",
    "    stock_list = pd.read_html( 'https://en.wikipedia.org/wiki/EURO_STOXX_50')[4]['Ticker'].to_list()\n",
    "\n",
    "    futures = pd.DataFrame()  \n",
    "\n",
    "    # xét từng mã\n",
    "    for symbol in stock_list:\n",
    "        try:\n",
    "            df = yf.Ticker(symbol).history(period = time_range, start = start_time, end = end_time)\n",
    "            df = pd.DataFrame(df['Close'])\n",
    "            df.columns = [symbol]\n",
    "            df.index = df.index.date\n",
    "            futures = pd.concat([futures,df],axis = 1, join = 'outer').sort_index()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    futures['Date'] = pd.to_datetime(futures.index, format='%Y-%m-%d')\n",
    "    futures.set_index('Date', inplace=True)\n",
    "\n",
    "    return futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classic TSMOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm thực hiện tính toán để lấy về giá trị volatility (biến động) của mỗi ngày"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Volatility_scale(data, ignore_na=False, adjust = True, com = 60, min_periods=0):\n",
    "    \"\"\"Scale data using ex ante volatility\"\"\"\n",
    "\n",
    "    # Lưu trữ index, tức thời gian \n",
    "    std_index = data.index\n",
    "\n",
    "    # chứa kết quả\n",
    "    daily_index = pd.DataFrame(index=std_index)\n",
    "\n",
    "    # xét từng cổ phiếu\n",
    "    for oo in data.columns:\n",
    "        returns = data[oo]  # Lấy ra các return\n",
    "        returns.dropna(inplace=True)  # xử lý null bằng zero\n",
    "\n",
    "        returns = returns.rolling(2).apply(lambda x: x.iloc[1] / x.iloc[0] - 1)\n",
    "        returns.iloc[0] =  0\n",
    "\n",
    "        # Tính cumulative (cum) return , nhưng ko có thành phần - 1\n",
    "        ret_index = (1 + returns).cumprod()\n",
    "\n",
    "        # Tính daily volatility (vol)\n",
    "        day_vol = returns.ewm(ignore_na=ignore_na,\n",
    "                              adjust=adjust,\n",
    "                              com=com,\n",
    "                              min_periods=min_periods).std(bias=False)\n",
    "        \n",
    "        vol = day_vol * np.sqrt(252)  # scale lại theo 252 ngày active trading\n",
    "\n",
    "        # Join cum return và vol\n",
    "        ret_index = pd.concat([ret_index, vol], axis=1)\n",
    "        ret_index.columns = [oo, oo + '_Vol']  # Đặt tên cột cum return là tên cổ phiếu, bên cạnh là vol \n",
    "\n",
    "        # Join \n",
    "        daily_index = pd.concat([daily_index, ret_index], join = 'outer' ,axis=1)\n",
    "\n",
    "    return daily_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm implement chiến lược TSMOM, với logic cụ thể như sau:\n",
    "Tại ngày t ta so  với ngày t - k về trước, cụ thể ta có thể lấy giá close,\n",
    " hoặc cumulative return (nhưng không có thành phần - 1, tức $\\text{cum return}_t = \\prod_{i = 0}^{t} (1 + r_i)$), \n",
    "ở đây xét `cum_return_t` với của k ngày trước\n",
    "`cum_return_{t-k}`\n",
    "  - Giả sử `cum_return_t` > `cum_return_{t-k}` tức `sign(cum_return_t - cum_return_{t - k}) = 1` (hàm dấu trả về 1 nếu input > 0)  thì ta có signal = 1, tức đó là tín hiệu để vào lệnh long vào ngày mai \n",
    "(ngày t + 1), ngược lại thì signal = -1, là tín hiệu vào short\n",
    "  -  Sau đó hold trong h -1 ngày tiếp theo (ngày t + 1 vào long đã bắt đầu tính là hold). \n",
    "  - Trong các ngày này (tức t + i với i từ 1 đến h), đều có sinh ra Profit and Loss (PnL)  tính theo công thức:\\\n",
    " ` 0.4/ vol_t * return_{t, t + i}` với `return_{t, t + i}` là return trong giai đoạn t đến t + i, tính tùy vào trường hợp long hay short:\n",
    "      - nếu long, `return_{t, t + i}` = 1 - `cum_return_t / cum_return_{t + i}`\n",
    "      - nếu short, `return_{t, t + i}` =  1 - `cum_return_{t + i} / cum_return_t` \n",
    "      \n",
    "    và Leverage, là ` target_vol / vol_t`   (target_vol đang để là 0.4)\n",
    " \n",
    " Tóm lại, Các kết quả trả về lần lượt là: \n",
    "- profit and loss `pnl` \n",
    "- `leverage`\n",
    "- `signal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classic_TSMOM(data, k, h, tolerance = 0,ignore_na = False, adjust = True, com = 60, min_periods = 0):\n",
    "    \n",
    "    signal = pd.DataFrame(index = data.index)\n",
    "\n",
    "    company = data.columns\n",
    "\n",
    "    # gọi hàm Volatility scale\n",
    "    daily_index = Volatility_scale(data,ignore_na=ignore_na,\n",
    "                          adjust=adjust,\n",
    "                          com=com,   \n",
    "                          min_periods = min_periods)\n",
    "\n",
    "\n",
    "    for oo in company:\n",
    "        flag_h = 0\n",
    "        flag_k = k+1\n",
    "        df = pd.concat([daily_index[oo], daily_index[oo+\"_Vol\"]], axis=1)\n",
    "        df = df.dropna(axis = 0, how = 'all')\n",
    "        df['rolling returns'] = df[oo].pct_change(k) # so sánh thay đổi ở ngày t với k ngày trước đó (tức t - k)\n",
    "        df['signal'] = 0.\n",
    "        for x, v in enumerate(df['rolling returns']):\n",
    "            if flag_h != 0:\n",
    "                # Bỏ qua giai đoạn hold, tránh bị tính lặp lại\n",
    "                flag_h = flag_h - 1\n",
    "                continue\n",
    "            # Bỏ qua thời gian cty chưa được lên sàn (nêu có)\n",
    "            if df[oo].isnull().iloc[x] == False:\n",
    "                # bỏ qua k ngày đầu vì chưa đủ k lookback\n",
    "                if flag_k != 0:\n",
    "                    flag_k = flag_k - 1\n",
    "                    continue\n",
    "            else: continue\n",
    "            try:\n",
    "                if df['rolling returns'].iloc[x-1] < tolerance:\n",
    "                    for h_period in range(0,h):\n",
    "                        # rolling return < 0, short rồi giữ trong h ngày, tính pnl, leverage///\n",
    "                        df['signal'].iloc[x + h_period] = -1\n",
    "                \n",
    "                elif df['rolling returns'].iloc[x-1] > tolerance:\n",
    "                    for h_period in range(0,h):\n",
    "                        # rolling return > 0, long rồi giữ trong h ngày, tính pnl, leverage///\n",
    "                        df['signal'].iloc[x + h_period] = 1\n",
    "\n",
    "            except:pass\n",
    "            \n",
    "\n",
    "            # Đặt flag holding là h - 1, để qua vòng for mới bỏ qua ngày hold, tránh bị tính lặp lại\n",
    "            if df['rolling returns'].iloc[x-1] != tolerance: flag_h = h - 1\n",
    "\n",
    "        signal = pd.concat([signal, df['signal']], join = 'outer', axis=1)\n",
    "\n",
    "    signal.columns = data.columns\n",
    "    \n",
    "    return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MACD(data,period_fast,period_slow):\n",
    "    '''Hàm dùng để tính MACD'''\n",
    "    EMA_fast = pd.Series(\n",
    "        data.ewm(ignore_na=False, span=period_fast, adjust=True).mean()\n",
    "    )\n",
    "    EMA_Slow = pd.Series(\n",
    "        data.ewm(ignore_na=False, span=period_slow, adjust=True).mean()\n",
    "    )\n",
    "    return EMA_fast - EMA_Slow\n",
    "\n",
    "def MACD_normalized(data, period_fast, period_slow):\n",
    "    '''Hàm dùng để tính MACD được chuẩn hóa'''\n",
    "    macd = MACD(data, period_fast, period_slow)\n",
    "    ewm_std_63 = data.ewm(span=63).std()\n",
    "    q = macd / ewm_std_63\n",
    "    z = q / q.ewm(span=252).std()\n",
    "    return z\n",
    "\n",
    "def calculate_rsi(data, period  = 14):\n",
    "    '''Hàm dùng để tính RSI'''\n",
    "    # daily changes\n",
    "    delta = data.diff()\n",
    "\n",
    "    gain = (delta.where(delta > 0, 0))\n",
    "    loss = (-delta.where(delta < 0, 0))\n",
    "\n",
    "    # average gain and loss over the period\n",
    "    avg_gain = gain.rolling(window=period).mean()\n",
    "    avg_loss = loss.rolling(window=period).mean()\n",
    "\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_features_single_asset(df,k,h):\n",
    "    '''Hàm để xây dựng feature dựa trên chuỗi giá Close theo thời gian của 1 công ty\n",
    "        Input:\n",
    "            df: Giá close của 1 công ty bất kì\n",
    "            k,h: Cặp lookback, holding sử dụng để tạo feature\n",
    "    '''\n",
    "    # Bỏ dữ liệu null (nếu có) trong dữ liệu\n",
    "    df = df.dropna(how='any',axis=0)\n",
    "    # Xây dựng Return Daily\n",
    "    df[\"Return Daily\"] = df[\"Close\"].rolling(2).apply(lambda x: x.iloc[1] / x.iloc[0] - 1)\n",
    "    df[\"Return Daily\"].iloc[0] =  0\n",
    "    # Xây dựng Volatility\n",
    "    df[\"Volatility_Scale\"] = Volatility_scale(pd.DataFrame(df[\"Close\"]))[\"Close_Vol\"]\n",
    "    df['Cummulative Return'] = (1+ df['Return Daily']).cumprod(axis = 0)\n",
    "    df['Mean H Return'] = df[\"Return Daily\"].rolling(h+1).apply(lambda x: x.iloc[range(1,h+1)].mean()).shift(-h)\n",
    "    df['Next H Return'] = df['Cummulative Return'].pct_change(h).shift(-h)\n",
    "    df['Square Sum Return'] = df[\"Return Daily\"].rolling(h+1).apply(lambda x: x.iloc[range(1,h+1)].pow(2).sum()).shift(-h)\n",
    "    df[\"Next H Vol\"] = df[\"Volatility_Scale\"].shift(-h)\n",
    "    df['Next H PnL'] = df['Next H Return'] / df[\"Next H Vol\"] \n",
    "\n",
    "    # Xây dựng feature cho lợi nhuận trong k ngày trước đó, lợi nhuận trong 1 ngày trước đó\n",
    "    for temp in [k,1]:\n",
    "        df[\"Past \" + str(temp) + \" Day\" ] = df['Close'].pct_change(temp) / (df[\"Volatility_Scale\"] * np.sqrt(252))\n",
    "    \n",
    "    df[\"MACD_19_39\"] = MACD_normalized(df[\"Close\"],19,39) # for longer trend\n",
    "    df[\"MACD_5_13\"] = MACD_normalized(df[\"Close\"], 5, 13) ## for fast trend\n",
    "\n",
    "    # RSI for overbought/oversold\n",
    "    df['RSI_5'] = calculate_rsi(df['Close'], period=5)\n",
    "\n",
    "    ## price sma\n",
    "    df['Price_SMA_5'] = df['Close'] / df['Close'].rolling(5).mean() - 1\n",
    "\n",
    "    # Label signal\n",
    "    df['Signal'] = [1 if x > 0 else 0 for x in df['Next H Return']]\n",
    "    \n",
    "    df = df.dropna(how='any',axis=0)\n",
    "    \n",
    "    # Cắt dữ liệu chỉ lấy các đoạn t; t +h; t+ 2h...\n",
    "    temp = pd.DataFrame(columns= df.columns)\n",
    "    n = 0\n",
    "    while True:\n",
    "        try:\n",
    "            temp = pd.concat([temp,df.iloc[[n*h],:]], axis = 0)\n",
    "            n = n+1\n",
    "        except: break\n",
    "    \n",
    "    try:\n",
    "        df = temp[:-2]\n",
    "    except:\n",
    "        df = None\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data,k,h,supervised = False, binary = True):\n",
    "    ''' Hàm để xây dựng feature cho nhiều công ty\n",
    "        Input: \n",
    "            data: Giá 'Close' của các cổ phiếu \n",
    "            k,h: Cặp lookback, holding sử dụng để tạo feature\n",
    "            supervised: Dùng để chỉ rõ việc lấy feature là để dùng cho mô hình có giám sát không\n",
    "            binary: Dùng để chỉ rõ việc lấy feature là để dùng cho mô hình phân loại không\n",
    "        Output: X , y cho việc huấn luyện mô luyện\n",
    "    '''\n",
    "    # Lấy tên các cổ phiếu\n",
    "    company = data.columns\n",
    "\n",
    "    # Ghi ra các feature sẽ lấy\n",
    "    features = []\n",
    "    for i in [k,1]:\n",
    "        features.append(\"Past \" + str(i) + \" Day\")\n",
    "    \n",
    "    features.append(\"MACD_19_39\")\n",
    "    features.append(\"MACD_5_13\")\n",
    "    features.append('RSI_5')\n",
    "    features.append('Price_SMA_5')\n",
    "    \n",
    "    X_train = pd.DataFrame(columns=features)\n",
    "\n",
    "    # Lấy các label sẽ sử dụng (tùy vào loại mô hình)\n",
    "    if supervised == False:\n",
    "        y_train = pd.DataFrame(columns=[\"Mean H Return\",\"Square Sum Return\",\"Volatility_Scale\"])\n",
    "    elif supervised == True:\n",
    "        if binary == True:\n",
    "            y_train = pd.DataFrame(columns=[\"Signal\"])\n",
    "        else:\n",
    "            y_train = pd.DataFrame(columns=[\"Next H PnL\"])\n",
    "    \n",
    "    # Chạy vòng lặp trên từng công ty\n",
    "    for oo in company:\n",
    "        df = data[[oo]].copy()\n",
    "        \n",
    "        # Lấy feature ở từng công ty\n",
    "        df.columns = [\"Close\"]\n",
    "        df = construct_features_single_asset(df,k,h)\n",
    "\n",
    "        if df is None:\n",
    "            continue\n",
    "        \n",
    "        X_train = pd.concat([X_train,df[features]],axis = 0)\n",
    "        if supervised == False:\n",
    "            y_train = pd.concat([y_train,df[[\"Mean H Return\",\"Square Sum Return\",\"Volatility_Scale\",\"Next H Return\"]]],axis = 0)\n",
    "        elif supervised == True:\n",
    "            if binary == True:\n",
    "                y_train = pd.concat([y_train,df[[\"Signal\"]]],axis = 0)\n",
    "            else:\n",
    "                y_train = pd.concat([y_train,df[[\"Next H PnL\"]]],axis = 0)\n",
    "            \n",
    "    return [X_train,y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree(X_train,y_train,X_val,y_val,k,h):\n",
    "    '''Hàm dùng để huấn luyện mô hình decision tree\n",
    "        Input: \n",
    "            X_train,y_train,X_val,y_val: X,y để huấn luyện, validate\n",
    "        Output: Mô hình sau khi đã được huấn luyện\n",
    "    '''\n",
    "    # Các parameter\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7, 10, 15],\n",
    "        'min_samples_split': [2, 5, 10, 20],\n",
    "        'min_samples_leaf': [1, 2, 4, 8],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "\n",
    "    # Tạo mô hình xgboost\n",
    "    model = GridSearch(model = tree.DecisionTreeClassifier(random_state=42), param_grid = param_grid,parallelize=False)\n",
    "    \n",
    "    # Fit và validation mô hình\n",
    "    model.fit(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(X_train,y_train,X_val,y_val,k,h):\n",
    "    '''Hàm dùng để huấn luyện mô hình xgboost\n",
    "        Input: \n",
    "            X_train,y_train,X_val,y_val: X,y để huấn luyện, validate\n",
    "        Output: Mô hình sau khi đã được huấn luyện\n",
    "    '''\n",
    "    # Các parameter\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7, 9],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0]\n",
    "    }\n",
    "\n",
    "    # Tạo mô hình xgboost\n",
    "    model = GridSearch(model = xgb.XGBClassifier(\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    ), param_grid = param_grid,parallelize=False)\n",
    "    \n",
    "    # Fit và validation mô hình\n",
    "    model.fit(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP (supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_supervised(kt.HyperModel):\n",
    "    def __init__(self, k,binary):\n",
    "        self.k = k\n",
    "        self.binary = binary\n",
    "\n",
    "    def build(self,hp):\n",
    "        model = Sequential([\n",
    "            Dropout(0, input_shape=(6,)),\n",
    "            Dense(units=hp.Choice(f\"units\", [5, 20, 40]),activation = hp.Choice('activation', ['relu'])),\n",
    "            Dropout(rate=hp.Choice(\"dropout\", [0.1, 0.3, 0.5])),\n",
    "            Dense(1,activation = 'sigmoid' if self.binary else None),\n",
    "        ])\n",
    "\n",
    "        if self.binary == True:\n",
    "            loss = 'binary_crossentropy'\n",
    "        else: loss = tf.keras.metrics.RootMeanSquaredError()\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(\n",
    "                learning_rate=hp.Choice(\"learning_rate\", [1e-3, 1e-1, 1.0]),\n",
    "                clipnorm = hp.Choice(\"max_grad_norm\", [1e-2, 0.1, 1.0, 10.0])\n",
    "            ),\n",
    "            loss= loss,\n",
    "        )\n",
    "        return model\n",
    "    def fit(self, hp, model, *args, **kwargs):        \n",
    "\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [256,512,1024]),\n",
    "            **kwargs, epochs = 100, verbose=1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLP_supervised(X_train,y_train,X_val,y_val,k,h,binary = True):\n",
    "    '''Hàm dùng để huấn luyện mô hình MLP\n",
    "        Input: \n",
    "            X_train,y_train,X_val,y_val: X,y để huấn luyện, validate\n",
    "            binary: Dùng để chỉ rõ sử dụng mô hình phân loại (True) hay là hồi quy (False)\n",
    "        Output: \n",
    "            model: Mô hình sau khi đã được huấn luyện\n",
    "            history: lịch sử huấn luyện hàm train loss, val loss từng epoch\n",
    "    '''\n",
    "    # Tạo hàm để tune theo GridSearch\n",
    "    tuner = kt.GridSearch(\n",
    "        MLP_supervised(k = k,binary = binary),\n",
    "        objective=\"val_loss\",\n",
    "        overwrite=True,\n",
    "        directory=\"tuning_dir\",\n",
    "        project_name= f\"tune_MLP_supervised_{'binary' if binary else 'reg'}\",\n",
    "    )\n",
    "\n",
    "    # Early Stopping\n",
    "    es = EarlyStopping(monitor='val_loss', verbose=1, patience=25)\n",
    "\n",
    "    # Thư mục checkpoint\n",
    "    checkpoint_filepath = (\n",
    "        'Test/Data/checkpoint_mlp_sup_binary.model.keras' if binary\n",
    "        else 'Test/Data/checkpoint_mlp_sup_reg.model.keras'\n",
    "    )\n",
    "\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor = 'val_loss',\n",
    "    save_best_only=True, verbose = 1)    \n",
    "\n",
    "    # Tune mô hình\n",
    "    tuner.search(X_train, y_train, callbacks = [es],validation_data=(X_val, y_val))\n",
    "\n",
    "    hypermodel = MLP_supervised(k = k,binary = binary)\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "    model = hypermodel.build(best_hp)\n",
    "\n",
    "    # Huấn luyện mô hình dựa trên param tốt nhất\n",
    "    history = hypermodel.fit(best_hp,model,X_train, y_train,callbacks = [model_checkpoint_callback],validation_data = (X_val, y_val))\n",
    "    \n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso (supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lasso_supervised(kt.HyperModel):\n",
    "    def __init__(self, k,binary):\n",
    "        self.k = k\n",
    "        self.binary = binary\n",
    "\n",
    "    def build(self,hp):\n",
    "        model = Sequential([\n",
    "            Dense(1, input_shape = (6,),kernel_regularizer = l1(hp.Choice(\"l1_weight\", [1e-4, 1e-3, 1e-2, 0.1,])),activation= 'sigmoid' if self.binary else None)\n",
    "        ])\n",
    "        \n",
    "        if self.binary == True:\n",
    "            loss = 'binary_crossentropy'\n",
    "        else: loss = tf.keras.metrics.RootMeanSquaredError()\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(\n",
    "                learning_rate=hp.Choice(\"learning_rate\", [1e-3, 1e-1, 1.0]),\n",
    "                clipnorm = hp.Choice(\"max_grad_norm\", [1e-2, 0.1, 1.0, 10.0])\n",
    "            ),\n",
    "            loss= loss,\n",
    "        )\n",
    "        return model\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "    \n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [256,512,1024]),\n",
    "            **kwargs, epochs = 100, verbose=1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Lasso_supervised(X_train,y_train,X_val,y_val,k,h,binary = True):\n",
    "    '''Hàm dùng để huấn luyện mô hình Lasso\n",
    "        Input: \n",
    "            X_train,y_train,X_val,y_val: X,y để huấn luyện, validate\n",
    "            binary: Dùng để chỉ rõ sử dụng mô hình phân loại (True) hay là hồi quy (False)\n",
    "        Output: \n",
    "            model: Mô hình sau khi đã được huấn luyện\n",
    "            history: lịch sử huấn luyện hàm train loss, val loss từng epoch\n",
    "    '''\n",
    "\n",
    "    # Tạo hàm tune theo GridSearch\n",
    "    tuner = kt.GridSearch(\n",
    "        Lasso_supervised(k = k,binary=binary),\n",
    "        objective=\"val_loss\",\n",
    "        overwrite=True,\n",
    "        directory=\"tuning_dir\",\n",
    "        project_name= f\"tune_Lasso_supervised_{'binary' if binary else 'reg'}\",\n",
    "\n",
    "    )\n",
    "\n",
    "    # Early Stopping\n",
    "    es = EarlyStopping(monitor='val_loss', verbose=1, patience=25)\n",
    "\n",
    "    # Thư mục checkpoint\n",
    "    checkpoint_filepath = (\n",
    "        'Test/Data/checkpoint_lasso_sup_binary.model.keras' if binary\n",
    "        else 'Test/Data/checkpoint_lasso_sup_reg.model.keras'\n",
    "    )\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor = 'val_loss',\n",
    "    save_best_only=True)    \n",
    "\n",
    "    # Tune mô hình\n",
    "    tuner.search(X_train, y_train, callbacks = [es],validation_data=(X_val, y_val))\n",
    "\n",
    "    hypermodel = Lasso_supervised(k = k,binary=binary)\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "    model = hypermodel.build(best_hp)\n",
    "\n",
    "    # Huấn luyện mô hình dựa trên param tốt nhất\n",
    "    history = hypermodel.fit(best_hp,model,X_train, y_train,callbacks = [model_checkpoint_callback],validation_data = (X_val, y_val))\n",
    "    \n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP (Sharpe Loss optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_loss(h,target_vol = 0.2):\n",
    "    ''' Tính toán hàm mất mát dựa theo sharpe Ratio\n",
    "        Input:\n",
    "            h: Thời gian holding\n",
    "            target_vol: Biến động mục tiêu\n",
    "            y_target_dummy: Ma trận (3,); bao gồm 3 feature dùng để thuận tiện cho tính toán\n",
    "            y_pred: Kết quả mô hình dự đoán\n",
    "        Output: Kết quả hàm loss\n",
    "    '''\n",
    "    def calculation(y_target_dummy, y_pred):\n",
    "        \n",
    "        # Tách các feature ra từ y_target_dummy\n",
    "        mean = K.reshape(y_target_dummy[:, 0], (-1, 1))\n",
    "        square_sum =  K.reshape(y_target_dummy[:, 1], (-1, 1))\n",
    "        volatility_scale = K.reshape(y_target_dummy[:, 2], (-1, 1))\n",
    "\n",
    "        # Tính mean \n",
    "        sum_pofolio = mean * h * y_pred * target_vol / volatility_scale\n",
    "        mean_pofolio = K.mean(mean * h * y_pred * target_vol / volatility_scale) / h\n",
    "\n",
    "        # Tính std\n",
    "        std_pofolio = tf.math.sqrt(K.mean(square_sum * y_pred **2  * (target_vol / volatility_scale)**2\n",
    "                                          - 2 * sum_pofolio * mean_pofolio \n",
    "                                          + (mean_pofolio ** 2) * h)/h)\n",
    "\n",
    "\n",
    "        return  - (mean_pofolio / std_pofolio) *np.sqrt(252) \n",
    "    return calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_SharpeLoss(kt.HyperModel):\n",
    "    def __init__(self, k,h):\n",
    "        self.k = k\n",
    "        self.h = h\n",
    "\n",
    "    def build(self,hp):\n",
    "        model = Sequential([\n",
    "            Dropout(0, input_shape=(6,)),\n",
    "            Dense(units=hp.Choice(f\"units\", [5, 20]),activation = hp.Choice('activation', ['tanh', 'relu'])),\n",
    "            Dropout(rate=hp.Choice(\"dropout\", [0.1, 0.3, 0.5])),\n",
    "            Dense(1,activation = 'tanh'),\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(\n",
    "                learning_rate=hp.Choice(\"learning_rate\", [1e-4, 1e-3, 1e-2, 1e-1, 1.0]),\n",
    "                clipnorm = hp.Choice(\"max_grad_norm\", [1e-2, 0.1, 1.0, 10.0])\n",
    "            ),\n",
    "            loss= sharpe_loss(h = self.h)\n",
    "        )\n",
    "        return model\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [256,512]),\n",
    "            **kwargs, epochs = 100, verbose=1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLP_sharpeLoss(X_train,y_train,X_val,y_val,k,h):\n",
    "    '''Hàm dùng để huấn luyện mô hình MLP\n",
    "        Input: \n",
    "            X_train,y_train,X_val,y_val: X,y để huấn luyện, validate\n",
    "        Output: \n",
    "            model: Mô hình sau khi đã được huấn luyện\n",
    "            history: lịch sử huấn luyện hàm train loss, val loss từng epoch\n",
    "    '''\n",
    "    tuner = kt.GridSearch(\n",
    "        MLP_SharpeLoss(k = k,h=h),\n",
    "        objective=\"val_loss\",\n",
    "        overwrite=True,\n",
    "        directory=\"tuning_dir\",\n",
    "        project_name=\"tune_MLP_sharpeLoss\",\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', verbose=1, patience=25)\n",
    "\n",
    "    checkpoint_filepath = 'Test/Data/checkpoint_MLP_sharpeLoss.model.keras'\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor = 'val_loss',\n",
    "    save_best_only=True)    \n",
    "\n",
    "\n",
    "    tuner.search(X_train, y_train, callbacks = [es], validation_data=(X_val, y_val))\n",
    "\n",
    "    hypermodel = MLP_SharpeLoss(k = k,h= h)\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "    model = hypermodel.build(best_hp)\n",
    "\n",
    "    history = hypermodel.fit(best_hp,model,X_train, y_train,callbacks = [model_checkpoint_callback],validation_data = (X_val, y_val))\n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso (Sharpe Loss optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lasso_SharpeLoss(kt.HyperModel):\n",
    "    def __init__(self, k,h):\n",
    "        self.k = k\n",
    "        self.h = h\n",
    "\n",
    "    def build(self,hp):\n",
    "        model = Sequential([\n",
    "            Dense(1, input_shape = (6,),kernel_regularizer = l1(hp.Choice(\"l1_weight\", [1e-3, 1e-2, 0.1,])),activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(\n",
    "                learning_rate=hp.Choice(\"learning_rate\", [1e-3, 1e-1, 1.0]),\n",
    "                clipnorm = hp.Choice(\"max_grad_norm\", [1e-2, 0.1, 1.0, 10.0])\n",
    "            ),\n",
    "            loss= sharpe_loss(h = self.h)\n",
    "        )\n",
    "        return model\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [256,512]),\n",
    "            **kwargs, epochs = 100, verbose=1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Lasso_sharpeLoss(X_train,y_train,X_val,y_val,k,h):\n",
    "    '''Hàm dùng để huấn luyện mô hình Lasso\n",
    "        Input: \n",
    "            X_train,y_train,X_val,y_val: X,y để huấn luyện, validate\n",
    "        Output: \n",
    "            model: Mô hình sau khi đã được huấn luyện\n",
    "            history: lịch sử huấn luyện hàm train loss, val loss từng epoch\n",
    "    '''\n",
    "    tuner = kt.GridSearch(\n",
    "        Lasso_SharpeLoss(k = k,h=h),\n",
    "        objective=\"val_loss\",\n",
    "        overwrite=True,\n",
    "        directory=\"tuning_dir\",\n",
    "        project_name=\"tune_Lasso_sharpeLoss\",\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', verbose=1, patience=25)\n",
    "\n",
    "    checkpoint_filepath = 'Test/Data/checkpoint_Lasso_sharpeLoss.model.keras'\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor = 'val_loss',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "    tuner.search(X_train, y_train, callbacks = [es], validation_data=(X_val, y_val))\n",
    "\n",
    "    hypermodel = Lasso_SharpeLoss(k = k,h= h)\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "    model = hypermodel.build(best_hp)\n",
    "\n",
    "    history = hypermodel.fit(best_hp,model,X_train, y_train,callbacks = [model_checkpoint_callback], validation_data = (X_val, y_val))\n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_TSMOM(data, model,k,h):\n",
    "    ''' Hàm dùng để lấy tín hiệu giao dịch trên các model đã được huấn luyện\n",
    "        Input:\n",
    "           data: Giá 'Close' của các cổ phiếu muốn giao dịch\n",
    "           model: mô hình đã được huấn luyện\n",
    "           k,h: Cặp k,h được sử dụng\n",
    "        Output: 1 file csv bao gồm tín hiệu giao dịch trên các công ty\n",
    "    '''\n",
    "    # Lấy tên các cổ phiếu\n",
    "    company = data.columns\n",
    "\n",
    "    # Tạo bảng signal\n",
    "    signal = pd.DataFrame(index = data.index)\n",
    "\n",
    "    # Ghi ra các feature sẽ lấy\n",
    "    features = []\n",
    "    for i in [k,1]:\n",
    "        features.append(\"Past \" + str(i) + \" Day\")\n",
    "\n",
    "    features.append(\"MACD_19_39\")\n",
    "    features.append(\"MACD_5_13\")\n",
    "    features.append('RSI_5')\n",
    "    features.append('Price_SMA_5')\n",
    "\n",
    "    # Chạy vòng lặp trên từng công ty\n",
    "    for oo in company:\n",
    "        df = data[[oo]].copy()\n",
    "        \n",
    "        # Lấy feature ở từng công ty\n",
    "        df.columns = [\"Close\"]\n",
    "        df = construct_features_single_asset(df,k,h,test= True)\n",
    "\n",
    "        if df is None: continue\n",
    "        time_index = data[oo].dropna(how = 'any').index\n",
    "        company_signal = pd.DataFrame(index = time_index, columns = [oo])\n",
    "        \n",
    "        X_test = df[features]\n",
    "        if X_test.shape[0] == 0: continue\n",
    "\n",
    "        # Chạy mô hình dự đoán\n",
    "        # Take signal\n",
    "        try:\n",
    "            if model.loss ==  'binary_crossentropy':\n",
    "                X_test['prediction'] = np.sign(model.predict(X_test) - 0.5)\n",
    "            else:\n",
    "                X_test['prediction'] = np.sign(model.predict(X_test))\n",
    "        except:\n",
    "            X_test['prediction'] = np.sign(model.predict(X_test))\n",
    "            X_test['prediction'][X_test['prediction'] == 0] = -1\n",
    "\n",
    "        # Take Direct Output\n",
    "        # try:\n",
    "        #     if model.loss ==  'binary_crossentropy':\n",
    "        #         X_test['prediction'] = np.sign(model.predict(X_test) - 0.5)\n",
    "        #         X_test['prediction'][X_test['prediction'] == 0] = -1\n",
    "        #     else:\n",
    "        #         X_test['prediction'] = model.predict(X_test)\n",
    "        # except:\n",
    "        #     X_test['prediction'] = model.predict(X_test)\n",
    "        \n",
    "        # for x,v in enumerate(X_test.index):\n",
    "        #     company_signal.loc[v,oo] = X_test.loc[v,'prediction']\n",
    "        \n",
    "        company_signal = company_signal.ffill()\n",
    "        company_signal = company_signal.fillna(0)\n",
    "\n",
    "        signal = pd.concat([signal,company_signal], axis = 1, join = 'outer')\n",
    "\n",
    "\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_history(history, model_name):\n",
    "    ''' Vẽ ra lịch sử huấn luyện của model (đối với các model trong thư viện keras)\n",
    "        Input:\n",
    "            history: file nhận được sau khi .fit() các model keras, lưu lại lịch sử train loss, val loss của model\n",
    "            model_name: tên model\n",
    "        Output: Lưu lại bức hình có tiêu đề model_name + \"loss\" với đồ thị biểu hiện hàm train loss, val loss của model\n",
    "    '''\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    # plot lịch sử của train_loss và val_loss\n",
    "    ax.plot(history.history['loss'])\n",
    "    ax.plot(history.history['val_loss'])\n",
    "\n",
    "    # Cài đặt đồ thị\n",
    "    ax.set_title(str(model_name) + ' loss')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.legend(['train', 'validation'], loc='upper left')\n",
    "    fig.savefig(str(model_name) + 'loss.png')\n",
    "    del fig\n",
    "    del ax\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(data,signal,k,h,  vol_flag = 1, target_vol = 0.2, ignore_na = False, adjust = True, com = 60, min_periods = 0):\n",
    "    ''' Bactesting chiến thuật khi đã có tín hiệu trong ngày, data cà cặp k,h định sử dụng\n",
    "        Input:\n",
    "            data: Giá 'Close' của các cổ phiếu \n",
    "            signal: Tín hiệu long/short qua từng ngày trên các cổ phiếu\n",
    "            k,h (int): Cặp k,h được sử dụng\n",
    "        Output: List gồm 2 dữ liệu chính:\n",
    "            pnl: Lời/lỗ hằng ngày khi thực hiện giao dịch trên các cổ phiếu\n",
    "            leverage: Khối lượng giao dịch trên các cổ phiếu đó, nếu vol_flag = 0 thì leverage sẽ là 1 bảng toàn 1\n",
    "    '''\n",
    "    pnl = pd.DataFrame(index=data.index)\n",
    "    leverage = pd.DataFrame(index = data.index)\n",
    "\n",
    "    company = signal.columns\n",
    "\n",
    "    # gọi hàm Volatility scale\n",
    "    daily_index = Volatility_scale(data,ignore_na=ignore_na,\n",
    "                          adjust=adjust,\n",
    "                          com=com,   \n",
    "                          min_periods = min_periods)\n",
    "\n",
    "\n",
    "    # Volatility settings\n",
    "    vol_flag = vol_flag    # Set flag to 1 for vol targeting\n",
    "    if vol_flag == 1:\n",
    "        target_vol = target_vol \n",
    "    else:\n",
    "        target_vol = 'no target vol'\n",
    "    \n",
    "\n",
    "    for oo in company:\n",
    "        flag_h = 0\n",
    "        flag_k = k+1\n",
    "        df = pd.concat([daily_index[oo], daily_index[oo+\"_Vol\"]], axis=1)\n",
    "        df = df.dropna(axis = 0, how = 'all')\n",
    "\n",
    "        company_signal = signal[oo].dropna(axis = 0, how = 'all')\n",
    "        df['pnl'] = 0. \n",
    "        df['leverage'] = 0.\n",
    "        for x, v in enumerate(df['pnl']):\n",
    "            if flag_h != 0:\n",
    "                # Bỏ qua giai đoạn hold, tránh bị tính lặp lại\n",
    "                flag_h = flag_h - 1\n",
    "                continue\n",
    "            # Bỏ qua thời gian cty chưa được lên sàn (nêu có)\n",
    "            if df[oo].isnull().iloc[x] == False:\n",
    "                # bỏ qua k ngày đầu vì chưa đủ k lookback\n",
    "                if flag_k != 0:\n",
    "                    flag_k = flag_k - 1\n",
    "                    continue\n",
    "            else: continue\n",
    "            try:\n",
    "                if company_signal.iloc[x] == -1:\n",
    "                    for h_period in range(0,h):\n",
    "                        if vol_flag == 1:\n",
    "                            df['pnl'].iloc[x + h_period] = (1 - df[oo].iloc[x + h_period] / df[oo].iloc[x - 1 + h_period]) * \\\n",
    "                                target_vol / df[oo+\"_Vol\"].iloc[x -1] \n",
    "                            df['leverage'].iloc[x + h_period] = target_vol / df[oo+\"_Vol\"].iloc[x -1]\n",
    "                        else:\n",
    "                            df['pnl'].iloc[x + h_period] = (1 - df[oo].iloc[x + h_period] / df[oo].iloc[x - 1 + h_period])\n",
    "                            df['leverage'].iloc[x+h_period] = 1\n",
    "                elif company_signal.iloc[x] == 1:\n",
    "                    for h_period in range(0,h):\n",
    "                        if vol_flag == 1:\n",
    "                            df['pnl'].iloc[x + h_period] = (df[oo].iloc[x + h_period] / df[oo].iloc[x - 1 + h_period] - 1) * \\\n",
    "                                    target_vol / df[oo+\"_Vol\"].iloc[x - 1]\n",
    "                            df['leverage'].iloc[x+h_period] = target_vol / df[oo+\"_Vol\"].iloc[x -1]\n",
    "                        else:\n",
    "                            df['pnl'].iloc[x + h_period] = (df[oo].iloc[x + h_period] / df[oo].iloc[x - 1 + h_period] - 1)\n",
    "                            df['leverage'].iloc[x+h_period] = 1\n",
    "            except:pass\n",
    "            \n",
    "            if signal[oo].iloc[x] == 1 or signal[oo].iloc[x] == -1 : flag_h = h - 1\n",
    "\n",
    "\n",
    "        leverage = pd.concat([leverage, df['leverage']], join = 'outer',axis = 1)\n",
    "        pnl = pd.concat([pnl, df['pnl']], join = 'outer',axis=1)\n",
    "\n",
    "    pnl.columns = signal.columns\n",
    "    leverage.columns = signal.columns\n",
    "\n",
    "    return [pnl,leverage]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuối cùng, ta lấy mean của 50 cổ phiếu để có `PnL` đại diện "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_daily_return(pnl):\n",
    "    \n",
    "    return pnl.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Example Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Lấy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thời gian input theo dạng yyyy-mm-dd; với ví dụ ở dưới \n",
    "\n",
    "start_time = '2004-12-31'\n",
    "end_time = '2010-01-01' \n",
    "\n",
    "df = EU_Stock_data(start_time,end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Code demo Classic TSMOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy data thô\n",
    "start_time = '2009-12-31'\n",
    "end_time = '2024-12-22'\n",
    "\n",
    "daily_return = EU_Stock_data(start_time = start_time, end_time=end_time)\n",
    "\n",
    "# scale dữ liệu theo độ biến động\n",
    "daily_index = Volatility_scale(daily_return)\n",
    "\n",
    "# print ra result là pnl, leverage, signal của hàm backtest_strategy(), với k = 3, h = 3, target volatility = 0.4\n",
    "LOOKBACK = 3\n",
    "HOLDING = 3\n",
    "TARGET_VOL = 0.4\n",
    "\n",
    "signal = classic_TSMOM(daily_return,LOOKBACK,HOLDING)\n",
    "[pnl,leverage] = backtest(daily_return,signal,LOOKBACK,HOLDING, target_vol= TARGET_VOL)\n",
    "\n",
    "print(f'pnl với k = {LOOKBACK} , h = {HOLDING}, target volatility = {TARGET_VOL}:')\n",
    "pnl\n",
    "\n",
    "print(f'leverage với k = {LOOKBACK} , h = {HOLDING}, target volatility = {TARGET_VOL}:')\n",
    "leverage\n",
    "\n",
    "print(f'signal với k = {LOOKBACK} , h = {HOLDING}, target volatility = {TARGET_VOL}:')\n",
    "signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Code so sánh các cặp k,h khi sử dụng classic TSMOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy dữ liệu thô\n",
    "start_time = '2019-12-31'\n",
    "end_time = '2024-12-31'\n",
    "\n",
    "data = EU_Stock_data(start_time = start_time, end_time=end_time)\n",
    "\n",
    "for k in range(1,11):\n",
    "    for h in range(1,11):\n",
    "        # print([k,h]) # Kiểm tra tiến độ\n",
    "\n",
    "        # backtest\n",
    "        signal = classic_TSMOM(data,k,h)\n",
    "        result = backtest(data,signal,k,h)\n",
    "        temp = strategy_daily_return(result[0])\n",
    "\n",
    "        # Thêm kết quả backtest của tất cả các cặp (k,h) được sử dụng về 1 file result.csvk_h_Comparing.csv\n",
    "        try:\n",
    "            temp2 = temp.to_list()\n",
    "            temp2.insert(0,h)\n",
    "            temp2.insert(0,k)\n",
    "            stats.loc[len(stats.index)] = temp2\n",
    "        except:\n",
    "            index = temp.index.to_list()\n",
    "            index.insert(0,'h')\n",
    "            index.insert(0,'k')\n",
    "            stats = pd.DataFrame(columns = index)\n",
    "            temp2 = temp.to_list()\n",
    "            temp2.insert(0,h)\n",
    "            temp2.insert(0,k)\n",
    "            stats.loc[len(stats.index)] = temp2\n",
    "        del result\n",
    "\n",
    "stats.to_csv(\"k_h_Comparing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Code demo thử model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cài đặt trước các chỉ số cho việc huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy data dựa trên thời gian bắt đầu và thời gian kết thúc\n",
    "\n",
    "start_time = '2018-12-31'\n",
    "end_time = '2024-12-22'\n",
    "\n",
    "# Lựa chọn trước cặp (k,h) để huấn luyện/ backtest mô hình\n",
    "k = 5\n",
    "h = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy dữ liệu thô\n",
    "data = EU_Stock_data(start_time = start_time, end_time=end_time)\n",
    "\n",
    "# Chọn lọc/ lấy các feature từ dữ liệu để huấn luyện mô hình\n",
    "X_binary, y_binary = feature_engineering(data,k,h,supervised= True)\n",
    "X_regression, y_regression = feature_engineering(data,k,h,supervised= True,binary= False)\n",
    "X_sharpeloss, y_sharpeloss = feature_engineering(data,k,h,supervised= False)\n",
    "\n",
    "test_data = data[data.index > datetime(pd.to_datetime(start_time).year + 5,12,31)]\n",
    "\n",
    "\n",
    "# Lấy tên các hàm ra để cho chạy vòng lặp huấn luyện mô hình\n",
    "model_name = ['classic_TSMOM','train_decision_tree','train_xgboost','train_MLP_supervised','train_Lasso_supervised','train_MLP_supervised_reg','train_Lasso_supervised_reg','train_MLP_sharpeLoss','train_Lasso_sharpeLoss']\n",
    "\n",
    "for model in model_name:\n",
    "    # print(model) # Kiểm tra tiến độ\n",
    "\n",
    "    # Thử nghiệm mô hình và backtest trên data test nếu sử dụng TSMOM\n",
    "    if model == 'classic_TSMOM':\n",
    "        func = globals()[model]\n",
    "        signal = func(test_data,k,h)\n",
    "        # signal.to_csv(\"signal_\" + str(model) + \".csv\")\n",
    "        pnl = strategy_daily_return(backtest(test_data,signal,k,h)[0])\n",
    "\n",
    "    # Lấy tập train/val từ X_regression để huấn luyện mô hình và backtest đối với mô hình hồi quy\n",
    "    elif model[-3:] == 'reg':\n",
    "        func = globals()[model[:-4]]\n",
    "\n",
    "        X_train = np.array(X_regression[X_regression.index <= datetime(pd.to_datetime(start_time).year + 4,12,31)], dtype=np.float64)\n",
    "        y_train = np.array(y_regression[y_regression.index <= datetime(pd.to_datetime(start_time).year + 4,12,31)], dtype=np.float64)\n",
    "\n",
    "        X_val = np.array(X_regression[(X_regression.index > datetime(pd.to_datetime(start_time).year + 4,12,31)) & (X_regression.index <= datetime(pd.to_datetime(start_time).year + 5,12,31))], dtype=np.float64)\n",
    "        y_val = np.array(y_regression[(y_regression.index > datetime(pd.to_datetime(start_time).year + 4,12,31)) & (y_regression.index <= datetime(pd.to_datetime(start_time).year + 5,12,31))], dtype=np.float64)\n",
    "        \n",
    "        # Huấn luyện mô hình\n",
    "        temp_model,history = func(X_train,y_train,X_val,y_val,k,h)\n",
    "        \n",
    "        # Lưu ảnh trực quan train/val loss khi huấn luyện mô hình\n",
    "        loss_history(history,str(model))\n",
    "        del history\n",
    "\n",
    "        # backtest\n",
    "        signal = test_model_TSMOM(test_data,temp_model,k,h)\n",
    "        # signal_1.to_csv(\"signal_\" + str(model) + \".csv\")\n",
    "        pnl = strategy_daily_return(backtest(test_data,signal,k,h)[0])\n",
    "        \n",
    "        del temp_model\n",
    "\n",
    "\n",
    "    # Lấy tập train/val từ X_binary để huấn luyện mô hình và backtest đối với các mô hình phân loại\n",
    "    elif model in ['train_decision_tree','train_xgboost','train_MLP_supervised','train_Lasso_supervised']:\n",
    "        func = globals()[model]\n",
    "        X_train = np.array(X_binary[X_binary.index <= datetime(pd.to_datetime(start_time).year + 4,12,31)], dtype=np.float64)\n",
    "        y_train = np.array(y_binary[y_binary.index <= datetime(pd.to_datetime(start_time).year + 4,12,31)], dtype=np.float64)\n",
    "\n",
    "        X_val = np.array(X_binary[(X_binary.index > datetime(pd.to_datetime(start_time).year + 4,12,31)) & (X_binary.index <= datetime(pd.to_datetime(start_time).year + 5,12,31))], dtype=np.float64)\n",
    "        y_val = np.array(y_binary[(y_binary.index > datetime(pd.to_datetime(start_time).year + 4,12,31)) & (y_binary.index <= datetime(pd.to_datetime(start_time).year + 5,12,31))], dtype=np.float64)\n",
    "\n",
    "        # Huấn luyện mô hình\n",
    "        if model in ['train_MLP_supervised','train_Lasso_supervised']:\n",
    "            temp_model,history = func(X_train,y_train,X_val,y_val,k,h)\n",
    "\n",
    "            # Lưu ảnh trực quan train/val loss khi huấn luyện mô hình đối với MLP; Lasso\n",
    "            loss_history(history,str(model))\n",
    "            del history\n",
    "        \n",
    "        else:\n",
    "            temp_model = func(X_train,y_train,X_val,y_val,k,h)\n",
    "\n",
    "        # backtest\n",
    "        signal = test_model_TSMOM(test_data,temp_model,k,h)\n",
    "        # signal.to_csv(\"signal_\" + str(model) + \".csv\")\n",
    "        pnl = strategy_daily_return(backtest(test_data,signal,k,h)[0])\n",
    "\n",
    "        del temp_model\n",
    "\n",
    "\n",
    "    # Lấy tập train/val từ X_binary để huấn luyện mô hình và backtest đối với các mô hình sử dụng hàm mất mát\n",
    "    else:\n",
    "        func = globals()[model]\n",
    "        X_train = np.array(X_sharpeloss[X_sharpeloss.index <= datetime(pd.to_datetime(start_time).year + 4,12,31)], dtype=np.float64)\n",
    "        y_train = np.array(y_sharpeloss[y_sharpeloss.index <= datetime(pd.to_datetime(start_time).year + 4,12,31)], dtype=np.float64)\n",
    "\n",
    "        X_val = np.array(X_sharpeloss[(X_sharpeloss.index > datetime(pd.to_datetime(start_time).year + 4,12,31)) & (X_sharpeloss.index <= datetime(pd.to_datetime(start_time).year + 5,12,31))], dtype=np.float64)\n",
    "        y_val = np.array(y_sharpeloss[(y_sharpeloss.index > datetime(pd.to_datetime(start_time).year + 4,12,31)) & (y_sharpeloss.index <= datetime(pd.to_datetime(start_time).year + 5,12,31))], dtype=np.float64)\n",
    "        \n",
    "        # Huấn luyện mô hình\n",
    "        temp_model,history = func(X_train,y_train,X_val,y_val,k,h)\n",
    "\n",
    "        # Lưu ảnh trực quan train/val loss khi huấn luyện mô hình\n",
    "        loss_history(history,str(model))\n",
    "        del history\n",
    "\n",
    "        #backtest\n",
    "        signal = test_model_TSMOM(test_data,temp_model,k,h)\n",
    "        # signal_1.to_csv(\"signal_\" + str(model) + \".csv\")\n",
    "        pnl = strategy_daily_return(backtest(test_data,signal,k,h)[0])\n",
    "        \n",
    "        del temp_model\n",
    "\n",
    "\n",
    "    # Thêm kết quả backtest của tất cả các mô hình được sử dụng về 1 file result.csv\n",
    "    try:\n",
    "        temp = pnl.to_list()\n",
    "        temp.insert(0,model)\n",
    "        stats.loc[len(stats.index)] = temp\n",
    "    except:\n",
    "        index = pnl.index.to_list()\n",
    "        index.insert(0,'Model')\n",
    "        stats = pd.DataFrame(columns = index)\n",
    "        temp = pnl.to_list()\n",
    "        temp.insert(0,model)\n",
    "        stats.loc[len(stats.index)] = temp\n",
    "\n",
    "\n",
    "stats.to_csv(\"result.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
