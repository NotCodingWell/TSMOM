{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Thêm thư viện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các thư viện cần thiết, trong đó có `yfinance` để lấy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from copy import deepcopy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "from sklearn import tree\n",
    "import xgboost as xgb\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Lấy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hàm lấy data, là cổ phiếu của 50 công ty trên sàn `EURO_STOXX_50` \\\n",
    "Kết quả trả về là 1 dataframe có dạng m dòng, 50 cột với m là time range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EU_Stock_data(start_time,end_time, time_range = 'max'):\n",
    "    \"\"\"Lấy dữ liệu giá Close của 50 công ty trên sàn Euro_STOXX 50 vào thời gian cho trước\"\"\"\n",
    "\n",
    "    stock_list = pd.read_html( 'https://en.wikipedia.org/wiki/EURO_STOXX_50')[4]['Ticker'].to_list()\n",
    "\n",
    "    futures = pd.DataFrame()  \n",
    "\n",
    "    # xét từng mã\n",
    "    for symbol in stock_list:\n",
    "        try:\n",
    "            df = yf.Ticker(symbol).history(period = time_range, start = start_time, end = end_time)\n",
    "            df = pd.DataFrame(df['Close']) # lấy giá close\n",
    "            i = 0\n",
    "            daily_return = []\n",
    "            # tinh daily return, = 0 trong ngày đầu tiên \n",
    "            for k in df['Close']:\n",
    "                if i != 0:\n",
    "                    daily_return.append(float((k-i)/i))\n",
    "                else:\n",
    "                    daily_return.append(float(0))\n",
    "                i = k\n",
    "            df['Close'] = daily_return\n",
    "            df.columns = [symbol]\n",
    "            df.index = df.index.date\n",
    "            futures = pd.concat([futures,df],axis = 1, join = 'outer').sort_index()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    futures['Date'] = pd.to_datetime(futures.index, format='%Y-%m-%d')\n",
    "    futures.set_index('Date', inplace=True)\n",
    "\n",
    "    return futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classic TSMOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm thực hiện tính toán để lấy về giá trị volatility (biến động) của mỗi ngày"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Volatility_scale(data, ignore_na=False, adjust = True, com = 60, min_periods=0):\n",
    "    \"\"\"Scale data using ex ante volatility\"\"\"\n",
    "\n",
    "    # Lưu trữ index, tức thời gian \n",
    "    std_index = data.index\n",
    "\n",
    "    # chứa kết quả\n",
    "    daily_index = pd.DataFrame(index=std_index)\n",
    "\n",
    "    # xét từng cổ phiếu\n",
    "    for oo in data.columns:\n",
    "        returns = data[oo]  # Lấy ra các return\n",
    "        returns.dropna(inplace=True)  # xử lý null bằng zero\n",
    "\n",
    "        # Tính cumulative (cum) return , nhưng ko có thành phần - 1\n",
    "        ret_index = (1 + returns).cumprod()\n",
    "\n",
    "        # Tính daily volatility (vol)\n",
    "        day_vol = returns.ewm(ignore_na=ignore_na,\n",
    "                              adjust=adjust,\n",
    "                              com=com,\n",
    "                              min_periods=min_periods).std(bias=False)\n",
    "        \n",
    "        vol = day_vol * np.sqrt(252)  # scale lại theo 252 ngày active trading\n",
    "\n",
    "        # Join cum return và vol\n",
    "        ret_index = pd.concat([ret_index, vol], axis=1)\n",
    "        ret_index.columns = [oo, oo + '_Vol']  # Đặt tên cột cum return là tên cổ phiếu, bên cạnh là vol \n",
    "\n",
    "        # Join \n",
    "        daily_index = pd.concat([daily_index, ret_index], axis=1)\n",
    "\n",
    "    return daily_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm implement chiến lược TSMOM, với logic cụ thể như sau:\n",
    "Tại ngày t ta so  với ngày t - k về trước, cụ thể ta có thể lấy giá close,\n",
    " hoặc cumulative return (nhưng không có thành phần - 1, tức $\\text{cum return}_t = \\prod_{i = 0}^{t} (1 + r_i)$), \n",
    "ở đây xét `cum_return_t` với của k ngày trước\n",
    "`cum_return_{t-k}`\n",
    "  - Giả sử `cum_return_t` > `cum_return_{t-k}` tức `sign(cum_return_t - cum_return_{t - k}) = 1` (hàm dấu trả về 1 nếu input > 0)  thì ta có signal = 1, tức đó là tín hiệu để vào lệnh long vào ngày mai \n",
    "(ngày t + 1), ngược lại thì signal = -1, là tín hiệu vào short\n",
    "  -  Sau đó hold trong h -1 ngày tiếp theo (ngày t + 1 vào long đã bắt đầu tính là hold). \n",
    "  - Trong các ngày này (tức t + i với i từ 1 đến h), đều có sinh ra Profit and Loss (PnL)  tính theo công thức:\\\n",
    " ` 0.4/ vol_t * return_{t, t + i}` với `return_{t, t + i}` là return trong giai đoạn t đến t + i, tính tùy vào trường hợp long hay short:\n",
    "      - nếu long, `return_{t, t + i}` = 1 - `cum_return_t / cum_return_{t + i}`\n",
    "      - nếu short, `return_{t, t + i}` =  1 - `cum_return_{t + i} / cum_return_t` \n",
    "      \n",
    "    và Leverage, là ` target_vol / vol_t`   (target_vol đang để là 0.4)\n",
    " \n",
    " Tóm lại, Các kết quả trả về lần lượt là: \n",
    "- profit and loss `pnl` \n",
    "- `leverage`\n",
    "- `signal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classic_TSMOM(data, k, h, tolerance = 0,ignore_na = False, adjust = True, com = 60, min_periods = 0):\n",
    "    signal = pd.DataFrame(index = data.index)\n",
    "\n",
    "    # gọi hàm Volatility scale\n",
    "    daily_index = Volatility_scale(data,ignore_na=ignore_na,\n",
    "                          adjust=adjust,\n",
    "                          com=com,   \n",
    "                          min_periods = min_periods)\n",
    "\n",
    "    company = data.columns\n",
    "\n",
    "    for oo in company:\n",
    "        flag_h = 0\n",
    "        flag_k = k+1\n",
    "        df = pd.concat([daily_index[oo], daily_index[oo+\"_Vol\"]], axis=1)\n",
    "        df = df.dropna(axis = 0, how = 'all')\n",
    "        df['rolling returns'] = df[oo].pct_change(k) # so sánh thay đổi ở ngày t với k ngày trước đó (tức t - k)\n",
    "        df['signal'] = 0.\n",
    "        for x, v in enumerate(df['rolling returns']):\n",
    "            if flag_h != 0:\n",
    "                # Bỏ qua giai đoạn hold, tránh bị tính lặp lại\n",
    "                flag_h = flag_h - 1\n",
    "                continue\n",
    "            # Bỏ qua thời gian cty chưa được lên sàn (nêu có)\n",
    "            if df[oo].isnull().iloc[x] == False:\n",
    "                # bỏ qua k ngày đầu vì chưa đủ k lookback\n",
    "                if flag_k != 0:\n",
    "                    flag_k = flag_k - 1\n",
    "                    continue\n",
    "            else: continue\n",
    "            try:\n",
    "                if df['rolling returns'].iloc[x-1] < tolerance:\n",
    "                    for h_period in range(0,h):\n",
    "                        # rolling return < 0, short rồi giữ trong h ngày, tính pnl, leverage///\n",
    "                        df['signal'].iloc[x + h_period] = -1\n",
    "                \n",
    "                elif df['rolling returns'].iloc[x-1] > tolerance:\n",
    "                    for h_period in range(0,h):\n",
    "                        # rolling return > 0, long rồi giữ trong h ngày, tính pnl, leverage///\n",
    "                        df['signal'].iloc[x + h_period] = 1\n",
    "\n",
    "            except:pass\n",
    "            \n",
    "\n",
    "            # Đặt flag holding là h - 1, để qua vòng for mới bỏ qua ngày hold, tránh bị tính lặp lại\n",
    "            if df['rolling returns'].iloc[x-1] != tolerance: flag_h = h - 1\n",
    "\n",
    "        signal = pd.concat([signal, df['signal']], join = 'outer', axis=1)\n",
    "\n",
    "    signal.columns = data.columns\n",
    "    \n",
    "    return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_features_single_asset(df,k,h, linear = False, test = False):\n",
    "    df = df.dropna(how='any',axis=0) \n",
    "    df['Cummulative Return'] = (1+ df['Return Daily']).cumprod(axis = 0)\n",
    "    df['Mean H Return'] = df[\"Return Daily\"].rolling(h+1).apply(lambda x: x.iloc[range(1,h+1)].mean()).shift(-h)\n",
    "    df['Next H Return'] = df['Cummulative Return'].pct_change(h).shift(-h)\n",
    "    df['Square Sum Return'] = df[\"Return Daily\"].rolling(h+1).apply(lambda x: x.iloc[range(1,h+1)].pow(2).sum()).shift(-h)\n",
    "\n",
    "    for temp in range(k,0,-1):\n",
    "        df[\"Before \" + str(temp) + \" Day\" ] = df['Cummulative Return'].rolling(h+1).apply(lambda x: x.iloc[h] / x.iloc[k - temp] - 1)\n",
    "\n",
    "    if linear == True:\n",
    "        df['Signal'] = [1 if x > 0 else -1 for x in df['Next H Return']]\n",
    "    \n",
    "    if test == True:\n",
    "        df = df.dropna(how='any',axis=0)\n",
    "        temp = pd.DataFrame(columns= df.columns)\n",
    "        n = 0\n",
    "        while True:\n",
    "            try:\n",
    "                temp = pd.concat([temp,df.iloc[[n*h],:]], axis = 0)\n",
    "                n = n+1\n",
    "            except: break\n",
    "        df = temp\n",
    "    else:\n",
    "        df = df.dropna(how='any',axis=0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data,k,h,linear = False):\n",
    "    company = data.columns\n",
    "    features = []\n",
    "    for i in range(k,0,-1):\n",
    "        features.append(\"Before \" + str(i) + \" Day\")\n",
    "\n",
    "    X_train = pd.DataFrame(columns=features)\n",
    "    if linear == False:\n",
    "        y_train = pd.DataFrame(columns=[\"Mean H Return\",\"Square Sum Return\"])\n",
    "    elif linear == True:\n",
    "        y_train = pd.DataFrame(columns=[\"Signal\"])\n",
    "    for oo in company:\n",
    "        df = data[[oo]].copy()\n",
    "        \n",
    "        df.columns = [\"Return Daily\"]\n",
    "\n",
    "        df = construct_features_single_asset(df,k,h,linear = linear)\n",
    "        \n",
    "        X_train = pd.concat([X_train,df[features]],axis = 0)\n",
    "        if linear == False:\n",
    "            y_train = pd.concat([y_train,df[[\"Mean H Return\",\"Square Sum Return\"]]],axis = 0)\n",
    "        elif linear == True:\n",
    "            y_train = pd.concat([y_train,df[[\"Signal\"]]],axis = 0)\n",
    "    return [X_train,y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree(data, k, h):\n",
    "\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "\n",
    "    X_train,y_train = feature_engineering(data,k,h,linear = True)\n",
    "    X_train = np.array(X_train, dtype=np.float64)\n",
    "    y_train = np.array(y_train, dtype=np.float64)\n",
    "    model= model.fit(X_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(data, k, h):\n",
    "\n",
    "    model = xgb.XGBRegressor(objective=\"multi:softmax\", num_class = 2,random_state=42)\n",
    "\n",
    "    X_train,y_train = feature_engineering(data,k,h,linear = True)\n",
    "    X_train = np.array(X_train, dtype=np.float64)\n",
    "    y_train = np.array(y_train, dtype=np.float64)\n",
    "    y_train[y_train == -1] = 0\n",
    "    model= model.fit(X_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP (supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_supervised(kt.HyperModel):\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def build(self,hp):\n",
    "        model = Sequential([\n",
    "            Dropout(0, input_shape=(self.k+1,)),\n",
    "            Dense(units=hp.Choice(f\"units\", [5, 10, 20, 40, 80]),activation = 'tanh'),\n",
    "            Dropout(rate=hp.Choice(\"dropout\", [0.1, 0.2, 0.3, 0.4, 0.5])),\n",
    "            Dense(1,activation = 'sigmoid'),\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(\n",
    "                learning_rate=hp.Choice(\"learning_rate\", [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]),\n",
    "                clipnorm = hp.Choice(\"max_grad_norm\", [1e-4, 1e-3, 1e-2, 0.1, 1.0, 10.0])\n",
    "            ),\n",
    "            loss=\"binary_crossentropy\",\n",
    "        )\n",
    "        return model\n",
    "    def fit(self, hp, model, *args, **kwargs):        \n",
    "\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [256,512,1024,2048]),\n",
    "            **kwargs, epochs = 100, verbose=1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLP_supervised(data, data_val,k, h):\n",
    "\n",
    "    X_train,y_train = feature_engineering(data,k,h,linear = True)\n",
    "    y_train[y_train == -1] = 0\n",
    "\n",
    "    X_train = np.array(X_train, dtype=np.float64)\n",
    "    y_train = np.array(y_train, dtype=np.float64)\n",
    "\n",
    "    X_val,y_val = feature_engineering(data_val,k,h,linear = True)\n",
    "    y_val[y_val == -1] = 0\n",
    "\n",
    "    X_val = np.array(X_val, dtype=np.float64)\n",
    "    y_val = np.array(y_val, dtype=np.float64)\n",
    "\n",
    "    \n",
    "    tuner = kt.RandomSearch(\n",
    "        MLP_supervised(k = k),\n",
    "        objective=\"val_loss\",\n",
    "        max_trials=50,\n",
    "        overwrite=True,\n",
    "        directory=\"tuning_dir\",\n",
    "        project_name=\"tune_MLP_supervised\",\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', verbose=1, patience=25)\n",
    "\n",
    "    checkpoint_filepath = 'Test/Data/checkpoint_MLP_supervised.model.keras'\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor = 'val_loss',\n",
    "    save_best_only=True)    \n",
    "\n",
    "    tuner.search(X_train, y_train, callbacks = [es],validation_data=(X_val, y_val))\n",
    "\n",
    "    hypermodel = MLP_supervised(k = k)\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "    model = hypermodel.build(best_hp)\n",
    "\n",
    "    history = hypermodel.fit(best_hp,model,X_train, y_train,callbacks = [model_checkpoint_callback],validation_data = (X_val, y_val))\n",
    "    \n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso (supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lasso_supervised(kt.HyperModel):\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def build(self,hp):\n",
    "        model = Sequential([\n",
    "            Dense(1, input_shape = (self.k+1,),kernel_regularizer = l1(hp.Choice(\"l1_weight\", [1e-5,1e-4, 1e-3, 1e-2, 0.1,])),activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(\n",
    "                learning_rate=hp.Choice(\"learning_rate\", [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]),\n",
    "                clipnorm = hp.Choice(\"max_grad_norm\", [1e-4, 1e-3, 1e-2, 0.1, 1.0, 10.0])\n",
    "            ),\n",
    "            loss=\"binary_crossentropy\",\n",
    "        )\n",
    "        return model\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "    \n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [256,512,1024,2048]),\n",
    "            **kwargs, epochs = 100, verbose=1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Lasso_supervised(data, data_val,k, h):\n",
    "\n",
    "\n",
    "    X_train,y_train = feature_engineering(data,k,h,linear = True)\n",
    "    y_train[y_train == -1] = 0\n",
    "    X_train = np.array(X_train, dtype=np.float64)\n",
    "    y_train = np.array(y_train, dtype=np.float64)\n",
    "\n",
    "    X_val,y_val = feature_engineering(data_val,k,h,linear = True)\n",
    "    y_val[y_val == -1] = 0\n",
    "\n",
    "    X_val = np.array(X_val, dtype=np.float64)\n",
    "    y_val = np.array(y_val, dtype=np.float64)\n",
    "    tuner = kt.RandomSearch(\n",
    "        Lasso_supervised(k = k),\n",
    "        objective=\"val_loss\",\n",
    "        max_trials=50,\n",
    "        overwrite=True,\n",
    "        directory=\"tuning_dir\",\n",
    "        project_name=\"tune_Lasso_supervised\",\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', verbose=1, patience=25)\n",
    "\n",
    "    checkpoint_filepath = 'Test/Data/checkpoint_Lasso_supervised.model.keras'\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor = 'val_loss',\n",
    "    save_best_only=True)    \n",
    "\n",
    "\n",
    "    tuner.search(X_train, y_train, callbacks = [es],validation_data=(X_val, y_val))\n",
    "\n",
    "    hypermodel = Lasso_supervised(k = k)\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "    model = hypermodel.build(best_hp)\n",
    "\n",
    "    history = hypermodel.fit(best_hp,model,X_train, y_train,callbacks = [model_checkpoint_callback],validation_data = (X_val, y_val))\n",
    "    \n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP (Sharpe Loss optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_loss(h):\n",
    "    def calculation(y_target_dummy, y_pred):\n",
    "\n",
    "        mean = K.reshape(y_target_dummy[:, 0], (-1, 1))\n",
    "        square_sum =  K.reshape(y_target_dummy[:, 1], (-1, 1))\n",
    "\n",
    "        sum_pofolio = mean * h * y_pred\n",
    "        mean_pofolio = K.mean(mean * h * y_pred) / h\n",
    "\n",
    "        std_pofolio = tf.math.sqrt(K.mean(square_sum * y_pred **2 \n",
    "                                          - 2 * sum_pofolio * mean_pofolio \n",
    "                                          + (mean_pofolio ** 2) * h)/h)\n",
    "\n",
    "    \n",
    "        return  - (mean_pofolio / std_pofolio) *np.sqrt(252)\n",
    "    \n",
    "    return calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_SharpeLoss(kt.HyperModel):\n",
    "    def __init__(self, k,h):\n",
    "        self.k = k\n",
    "        self.h = h\n",
    "\n",
    "    def build(self,hp):\n",
    "        model = Sequential([\n",
    "            Dropout(0, input_shape=(self.k+1,)),\n",
    "            Dense(units=hp.Choice(f\"units\", [5, 10, 20, 40, 80]),activation = 'tanh'),\n",
    "            Dropout(rate=hp.Choice(\"dropout\", [0.1, 0.2, 0.3, 0.4, 0.5])),\n",
    "            Dense(1,activation = 'tanh'),\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(\n",
    "                learning_rate=hp.Choice(\"learning_rate\", [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]),\n",
    "                clipnorm = hp.Choice(\"max_grad_norm\", [1e-4, 1e-3, 1e-2, 0.1, 1.0, 10.0])\n",
    "            ),\n",
    "            loss= sharpe_loss(h = self.h)\n",
    "        )\n",
    "        return model\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [256,512,1024,2048]),\n",
    "            **kwargs, epochs = 100, verbose=1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLP_sharpeLoss(data, data_val, k, h):\n",
    "\n",
    "    X_train,y_train = feature_engineering(data,k,h,linear = False)\n",
    "\n",
    "    X_train = np.array(X_train, dtype=np.float64)\n",
    "    y_train = np.array(y_train, dtype=np.float64)\n",
    "\n",
    "    X_val,y_val = feature_engineering(data_val,k,h,linear = False)\n",
    "\n",
    "    X_val = np.array(X_val, dtype=np.float64)\n",
    "    y_val = np.array(y_val, dtype=np.float64)\n",
    "    tuner = kt.RandomSearch(\n",
    "        MLP_SharpeLoss(k = k,h=h),\n",
    "        objective=\"val_loss\",\n",
    "        max_trials=50,\n",
    "        overwrite=True,\n",
    "        directory=\"tuning_dir\",\n",
    "        project_name=\"tune_MLP_sharpeLoss\",\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', verbose=1, patience=25)\n",
    "\n",
    "    checkpoint_filepath = 'Test/Data/checkpoint_MLP_sharpeLoss.model.keras'\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor = 'val_loss',\n",
    "    save_best_only=True)    \n",
    "\n",
    "\n",
    "    tuner.search(X_train, y_train, callbacks = [es], validation_data=(X_val, y_val))\n",
    "\n",
    "    hypermodel = MLP_SharpeLoss(k = k,h= h)\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "    model = hypermodel.build(best_hp)\n",
    "\n",
    "    history = hypermodel.fit(best_hp,model,X_train, y_train,callbacks = [model_checkpoint_callback],validation_data = (X_val, y_val))\n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso (Sharpe Loss optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lasso_SharpeLoss(kt.HyperModel):\n",
    "    def __init__(self, k,h):\n",
    "        self.k = k\n",
    "        self.h = h\n",
    "\n",
    "    def build(self,hp):\n",
    "        model = Sequential([\n",
    "            Dense(1, input_shape = (self.k+1,),kernel_regularizer = l1(hp.Choice(\"l1_weight\", [1e-5,1e-4, 1e-3, 1e-2, 0.1,])),activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(\n",
    "                learning_rate=hp.Choice(\"learning_rate\", [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]),\n",
    "                clipnorm = hp.Choice(\"max_grad_norm\", [1e-4, 1e-3, 1e-2, 0.1, 1.0, 10.0])\n",
    "            ),\n",
    "            loss= sharpe_loss(h = self.h)\n",
    "        )\n",
    "        return model\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [256,512,1024,2048]),\n",
    "            **kwargs, epochs = 100, verbose=1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Lasso_sharpeLoss(data, data_val,k, h):\n",
    "\n",
    "    X_train,y_train = feature_engineering(data,k,h,linear = False)\n",
    "\n",
    "    X_train = np.array(X_train, dtype=np.float64)\n",
    "    y_train = np.array(y_train, dtype=np.float64)\n",
    "\n",
    "    X_val,y_val = feature_engineering(data_val,k,h,linear = False)\n",
    "    \n",
    "    X_val = np.array(X_val, dtype=np.float64)\n",
    "    y_val = np.array(y_val, dtype=np.float64)\n",
    "    tuner = kt.RandomSearch(\n",
    "        Lasso_SharpeLoss(k = k,h=h),\n",
    "        objective=\"val_loss\",\n",
    "        max_trials=50,\n",
    "        overwrite=True,\n",
    "        directory=\"tuning_dir\",\n",
    "        project_name=\"tune_Lasso_sharpeLoss\",\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', verbose=1, patience=25)\n",
    "\n",
    "    checkpoint_filepath = 'Test/Data/checkpoint_Lasso_sharpeLoss.model.keras'\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor = 'val_loss',\n",
    "    save_best_only=True)    \n",
    "\n",
    "\n",
    "    tuner.search(X_train, y_train, callbacks = [es], validation_data=(X_val, y_val))\n",
    "\n",
    "    hypermodel = Lasso_SharpeLoss(k = k,h= h)\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "    model = hypermodel.build(best_hp)\n",
    "\n",
    "    history = hypermodel.fit(best_hp,model,X_train, y_train,callbacks = [model_checkpoint_callback], validation_data = (X_val, y_val))\n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_TSMOM(data, model,k,h,linear = False):\n",
    "\n",
    "    company = data.columns\n",
    "\n",
    "    signal = pd.DataFrame(index = data.index)\n",
    "\n",
    "    features = []\n",
    "    for i in range(k,0,-1):\n",
    "        features.append(\"Before \" + str(i) + \" Day\")\n",
    "    features.append(\"Return Daily\")\n",
    "\n",
    "    for oo in company:\n",
    "        df = data[[oo]].copy()\n",
    "        \n",
    "        df.columns = [\"Return Daily\"]\n",
    "        df = construct_features_single_asset(df,k,h,linear = linear,test= True)\n",
    "        time_index = data[oo].dropna(how = 'any').index\n",
    "        company_signal = pd.DataFrame(index = time_index, columns = [oo])\n",
    "        \n",
    "        X_test = df[features]\n",
    "        if X_test.shape[0] == 0: continue\n",
    "        try:\n",
    "            if model.loss ==  'binary_crossentropy':\n",
    "                X_test['prediction'] = np.sign(model.predict(X_test) - 0.5)\n",
    "            else:\n",
    "                X_test['prediction'] = np.sign(model.predict(X_test))\n",
    "        except:\n",
    "            X_test['prediction'] = np.sign(model.predict(X_test))\n",
    "            X_test['prediction'][X_test['prediction'] == 0] = -1\n",
    "        for x,v in enumerate(X_test.index):\n",
    "            company_signal.loc[v,oo] = X_test.loc[v,'prediction']\n",
    "        \n",
    "        company_signal = company_signal.ffill()\n",
    "        company_signal = company_signal.fillna(0)\n",
    "\n",
    "        signal = pd.concat([signal,company_signal], axis = 1, join = 'outer')\n",
    "\n",
    "\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(data,signal,k,h,  vol_flag = 1, target_vol = 0.2, ignore_na = False, adjust = True, com = 60, min_periods = 0):\n",
    "    \n",
    "    pnl = pd.DataFrame(index=data.index)\n",
    "    leverage = pd.DataFrame(index = data.index)\n",
    "\n",
    "    # gọi hàm Volatility scale\n",
    "    daily_index = Volatility_scale(data,ignore_na=ignore_na,\n",
    "                          adjust=adjust,\n",
    "                          com=com,   \n",
    "                          min_periods = min_periods)\n",
    "\n",
    "    company = signal.columns\n",
    "\n",
    "    # Volatility settings\n",
    "    vol_flag = vol_flag    # Set flag to 1 for vol targeting\n",
    "    if vol_flag == 1:\n",
    "        target_vol = target_vol \n",
    "    else:\n",
    "        target_vol = 'no target vol'\n",
    "    \n",
    "\n",
    "    for oo in company:\n",
    "        flag_h = 0\n",
    "        flag_k = k+1\n",
    "        df = pd.concat([daily_index[oo], daily_index[oo+\"_Vol\"]], axis=1)\n",
    "        df = df.dropna(axis = 0, how = 'all')\n",
    "\n",
    "        company_signal = signal[oo].dropna(axis = 0, how = 'all')\n",
    "        df['pnl'] = 0. \n",
    "        df['leverage'] = 0.\n",
    "        for x, v in enumerate(df['pnl']):\n",
    "            if flag_h != 0:\n",
    "                # Bỏ qua giai đoạn hold, tránh bị tính lặp lại\n",
    "                flag_h = flag_h - 1\n",
    "                continue\n",
    "            # Bỏ qua thời gian cty chưa được lên sàn (nêu có)\n",
    "            if df[oo].isnull().iloc[x] == False:\n",
    "                # bỏ qua k ngày đầu vì chưa đủ k lookback\n",
    "                if flag_k != 0:\n",
    "                    flag_k = flag_k - 1\n",
    "                    continue\n",
    "            else: continue\n",
    "            try:\n",
    "                if company_signal.iloc[x] == -1:\n",
    "                    for h_period in range(0,h):\n",
    "                        if vol_flag == 1:\n",
    "                            df['pnl'].iloc[x + h_period] = (1 - df[oo].iloc[x + h_period] / df[oo].iloc[x - 1 + h_period]) * \\\n",
    "                                target_vol / df[oo+\"_Vol\"].iloc[x -1] \n",
    "                            df['leverage'].iloc[x + h_period] = target_vol / df[oo+\"_Vol\"].iloc[x -1]\n",
    "                        else:\n",
    "                            df['pnl'].iloc[x + h_period] = (1 - df[oo].iloc[x + h_period] / df[oo].iloc[x - 1 + h_period])\n",
    "                            df['leverage'].iloc[x+h_period] = 1\n",
    "                elif company_signal.iloc[x] == 1:\n",
    "                    for h_period in range(0,h):\n",
    "                        if vol_flag == 1:\n",
    "                            df['pnl'].iloc[x + h_period] = (df[oo].iloc[x + h_period] / df[oo].iloc[x - 1 + h_period] - 1) * \\\n",
    "                                    target_vol / df[oo+\"_Vol\"].iloc[x - 1]\n",
    "                            df['leverage'].iloc[x+h_period] = target_vol / df[oo+\"_Vol\"].iloc[x -1]\n",
    "                        else:\n",
    "                            df['pnl'].iloc[x + h_period] = (df[oo].iloc[x + h_period] / df[oo].iloc[x - 1 + h_period] - 1)\n",
    "                            df['leverage'].iloc[x+h_period] = 1\n",
    "            except:pass\n",
    "            \n",
    "            if signal[oo].iloc[x] == 1 or signal[oo].iloc[x] == -1 : flag_h = h - 1\n",
    "\n",
    "\n",
    "        leverage = pd.concat([leverage, df['leverage']], join = 'outer',axis = 1)\n",
    "        pnl = pd.concat([pnl, df['pnl']], join = 'outer',axis=1)\n",
    "\n",
    "    pnl.columns = signal.columns\n",
    "    leverage.columns = signal.columns\n",
    "\n",
    "    return [pnl,leverage]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuối cùng, ta lấy mean của 50 cổ phiếu để có `PnL` đại diện "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_daily_return(pnl):\n",
    "    \n",
    "    return pnl.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Example Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Lấy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Thời gian input theo dạng yyyy-mm-dd; với ví dụ ở dưới \n",
    "\n",
    "# start_time = '2004-12-31'\n",
    "# end_time = '2010-01-01' \n",
    "\n",
    "# df = EU_Stock_data(start_time,end_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Code demo Classic TSMOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_return = EU_Stock_data()\n",
    "daily_index = Volatility_scale(daily_return)\n",
    "\n",
    "# print ra result là pnl, leverage, signal của hàm backtest_strategy(), với k = 3, h = 3, target volatility = 0.4\n",
    "LOOKBACK = 3\n",
    "HOLDING = 3\n",
    "TARGET_VOL = 0.4\n",
    "\n",
    "signal = classic_TSMOM(daily_return,LOOKBACK,HOLDING)\n",
    "[pnl,leverage] = backtest(daily_return,signal,LOOKBACK,HOLDING, target_vol= TARGET_VOL)\n",
    "\n",
    "print(f'pnl với k = {LOOKBACK} , h = {HOLDING}, target volatility = {TARGET_VOL}:')\n",
    "pnl\n",
    "\n",
    "print(f'leverage với k = {LOOKBACK} , h = {HOLDING}, target volatility = {TARGET_VOL}:')\n",
    "leverage\n",
    "\n",
    "print(f'signal với k = {LOOKBACK} , h = {HOLDING}, target volatility = {TARGET_VOL}:')\n",
    "signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Code so sánh các cặp k,h khi sử dụng classic TSMOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv' , index_col= 'Date')\n",
    "\n",
    "for k in [1,3,5,10,15,20,40]:\n",
    "    for h in [1,3,5,10,15,20,40]:\n",
    "        # print([k,h]) # Kiểm tra tiến độ\n",
    "        signal = classic_TSMOM(data,k,h)\n",
    "        result = backtest(data,signal,k,h)\n",
    "        # result[0].to_csv(\"pnl (k = \" + str(0 if k < 10 else \"\") + str(k) + \", h = \" + str(0 if h < 10 else \"\") + str(h) + \").csv\")\n",
    "        # result[1].to_csv(\"leverage (k = \" + str(0 if k < 10 else \"\") + str(k) + \", h = \" + str(0 if h < 10 else \"\")  + str(h) + \").csv\")\n",
    "        # result[2].to_csv(\"signal (k = \" + str(0 if k < 10 else \"\") + str(k) + \", h = \" + str(0 if h < 10 else \"\")  + str(h) + \").csv\")\n",
    "        temp = strategy_daily_return(result[0])\n",
    "        try:\n",
    "            temp2 = temp.to_list()\n",
    "            temp2.insert(0,h)\n",
    "            temp2.insert(0,k)\n",
    "            stats.loc[len(stats.index)] = temp2\n",
    "        except:\n",
    "            index = temp.index.to_list()\n",
    "            index.insert(0,'h')\n",
    "            index.insert(0,'k')\n",
    "            stats = pd.DataFrame(columns = index)\n",
    "            temp2 = temp.to_list()\n",
    "            temp2.insert(0,h)\n",
    "            temp2.insert(0,k)\n",
    "            stats.loc[len(stats.index)] = temp2\n",
    "        del result\n",
    "\n",
    "stats.to_csv(\"k_h_Comparing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Code demo thử model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train theo tỉ lệ Data (9 năm Train; 1 năm Validation và 5 năm Backtest từ 15 năm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = '2009-12-31'\n",
    "end_time = '2024-12-22'\n",
    "\n",
    "data = EU_Stock_data(start_time = start_time, end_time=end_time)\n",
    "\n",
    "k = 40\n",
    "h = 40\n",
    "\n",
    "model_name = ['classic_TSMOM','train_decision_tree','train_xgboost','train_MLP_supervised','train_Lasso_supervised','train_MLP_sharpeLoss','train_Lasso_sharpeLoss']\n",
    "\n",
    "for model in model_name:\n",
    "    func = globals()[model]\n",
    "    if model == 'classic_TSMOM':\n",
    "        test_data = data[data.index > datetime(pd.to_datetime(start_time).year + 10,12,31)]\n",
    "\n",
    "        signal = func(test_data,k,h)\n",
    "        # signal.to_csv(\"signal_\" + str(model) + \".csv\")\n",
    "        pnl = strategy_daily_return(backtest(test_data,signal,k,h)[0])\n",
    "\n",
    "    elif model in ['train_decision_tree','train_xgboost']:\n",
    "        train_data = data[data.index <= datetime(pd.to_datetime(start_time).year + 10,12,31)]\n",
    "        test_data = data[data.index > datetime(pd.to_datetime(start_time).year + 10,12,31)]\n",
    "\n",
    "        temp_model = func(train_data,k,h)\n",
    "        signal = test_model_TSMOM(test_data,temp_model,k,h)\n",
    "        # signal.to_csv(\"signal_\" + str(model) + \".csv\")\n",
    "        pnl = strategy_daily_return(backtest(test_data,signal,k,h)[0])\n",
    "\n",
    "    else:\n",
    "        train_data = data[data.index <= datetime(pd.to_datetime(start_time).year + 9,12,31)]\n",
    "        val_data = data[data.index > datetime(pd.to_datetime(start_time).year + 9,12,31) and data.index <= datetime(pd.to_datetime(start_time).year + 10,12,31)]\n",
    "        test_data = data[data.index > datetime(pd.to_datetime(start_time).year + 10,12,31)]\n",
    "        temp_model = func(train_data,val_data,k,h)[0]\n",
    "        signal = test_model_TSMOM(test_data,temp_model,k,h)\n",
    "        # signal_1.to_csv(\"signal_\" + str(model) + \".csv\")\n",
    "        pnl = strategy_daily_return(backtest(test_data,signal,k,h)[0])\n",
    "\n",
    "    try:\n",
    "        if len(stats.columns) > 0:\n",
    "            temp = pnl\n",
    "            temp.insert(0,model)\n",
    "            stats.loc[len(stats.index)] = temp\n",
    "    except:\n",
    "        index = pnl.index.to_list()\n",
    "        index.insert(0,'Model')\n",
    "        stats = pd.DataFrame(columns = index)\n",
    "        temp = pnl\n",
    "        temp.insert(0,model)\n",
    "        stats.loc[len(stats.index)] = temp\n",
    "\n",
    "\n",
    "stats.to_csv(\"result.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train theo Rolling Window"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
